{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Prompt Engineering\n",
        "\n",
        "This notebook will guide you through the process of prompt engineering with OpenAI's ChatGPT model.\n"
      ],
      "metadata": {
        "id": "hzma7M56r3C_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Setup\n",
        "This section will guide you on how to set up the necessary OpenAI libraries in your development environment. These libraries provide the essential interfaces for interacting with OpenAI's models like ChatGPT.\n"
      ],
      "metadata": {
        "id": "dJfhMPefFipY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Installing required libraries"
      ],
      "metadata": {
        "id": "E-yxhl6iGWdi"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Gi_qGE98rzUY",
        "outputId": "75034d42-a13c-4a40-87b7-886d7376708c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: pip in /usr/local/lib/python3.10/dist-packages (23.1.2)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m73.6/73.6 kB\u001b[0m \u001b[31m2.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m19.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m114.5/114.5 kB\u001b[0m \u001b[31m10.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m268.8/268.8 kB\u001b[0m \u001b[31m17.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m149.6/149.6 kB\u001b[0m \u001b[31m3.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "!pip install --upgrade pip\n",
        "!pip install -q openai"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import openai"
      ],
      "metadata": {
        "id": "liZ3RgbosZQM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Connecting to OpenAI using a key\n",
        "Learn how to securely connect to OpenAI's API using the provided key. This ensures authorized access to OpenAI's services."
      ],
      "metadata": {
        "id": "U0I_Ad6xHMOY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#You can API key by registering in OpenAPI here https://platform.openai.com/account/api-keys..\n",
        "openai.api_key = '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'"
      ],
      "metadata": {
        "id": "Qc5iuxoZHLjL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Using ChatGPT via the endpoint\n",
        "Understand how to interact with the ChatGPT model using OpenAI's API endpoint. This includes how to send requests and handle responses from the model."
      ],
      "metadata": {
        "id": "7X7gYKziE7Ie"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "\n",
        "#here we define a function to use open-ai API end point. So that we can play with the parameters.\n",
        "# This model will use gpt-3.5-turbo(chatgpt) model in default\n",
        "def use_endpoint(msg, temperature=1, top_p=1, model = \"gpt-3.5-turbo\"):\n",
        "\n",
        "  if(model==\"text-davinci-003\"):\n",
        "    response = openai.Completion.create(\n",
        "        engine = \"text-davinci-003\",\n",
        "        prompt = msg,\n",
        "        temperature = temperature,\n",
        "        top_p = top_p,\n",
        "        max_tokens = 150,\n",
        "    )\n",
        "    return response.choices[0].text.strip()\n",
        "\n",
        "\n",
        "  URL = \"https://api.openai.com/v1/chat/completions\"\n",
        "\n",
        "  payload = {\n",
        "  \"model\": model,\n",
        "  \"messages\": [{\"role\": \"user\", \"content\": msg}],\n",
        "  \"temperature\" : temperature,\n",
        "  \"top_p\":top_p,\n",
        "  }\n",
        "\n",
        "  headers = {\n",
        "  \"Content-Type\": \"application/json\",\n",
        "  \"Authorization\": f\"Bearer {openai.api_key}\"\n",
        "  }\n",
        "\n",
        "  response = requests.post(URL, headers=headers, json=payload, stream=False)\n",
        "  response_json = response.json()\n",
        "  # print(response_json)\n",
        "  return response_json['choices'][0]['message']['content'].strip()"
      ],
      "metadata": {
        "id": "RGX2d_Xcr_wQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Parameters\n",
        "This section will detail the different parameters that can be adjusted when making requests to the ChatGPT model and how they influence the generated responses."
      ],
      "metadata": {
        "id": "2OUF_Tz7GK73"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Temperature and Top-p\n",
        "\n",
        "Both of these parameters decides the degree of deterministic nature of the language model.\n",
        "\n",
        "<img src=\"http://drive.google.com/uc?export=view&id=14gIyrIHTsJH4SOyHAk65cgT3ta2RKpSR\" />\n"
      ],
      "metadata": {
        "id": "mKdflGRVFEEb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#low temperature and high top-p\n",
        "prompt = \"create a tag line for a coffee shop\"\n",
        "for i in range(10):\n",
        "  response = use_endpoint(prompt, temperature=0, top_p=0)\n",
        "  print(response)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GsSR3QyoOGqv",
        "outputId": "166b3172-1267-4339-b1a8-0b7e1ba03d01"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\"Fuel your day with our brews, sip by sip.\"\n",
            "\"Fuel your day with our brews, sip by sip.\"\n",
            "\"Fuel your day with our brews, sip by sip.\"\n",
            "\"Fuel your day with our brews, sip by sip.\"\n",
            "\"Fuel your day with our brews, sip by sip.\"\n",
            "\"Fuel your day with our brews\"\n",
            "\"Fuel your day with our brews\"\n",
            "\"Fuel your day with our brews\"\n",
            "\"Fuel your day with our brews, sip by sip.\"\n",
            "\"Fuel your day with our brews, sip by sip.\"\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#high temperature and high top-p\n",
        "for i in range(10):\n",
        "  response = use_endpoint(prompt, temperature=1, top_p=1)\n",
        "  print(response)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lrzoPouSOptL",
        "outputId": "7eec67fb-390e-4205-c0ba-a88cb3b9c6bd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\"Awaken your senses, one sip at a time\"\n",
            "\"Fuel your day with our rich and bold brews.\"\n",
            "\"Fuel your day with our handcrafted brews\"\n",
            "\"Awaken your senses with every sip at our coffee shop.\"\n",
            "\"Fuel your day with our brews\"\n",
            "\"Fuel your day with the perfect brew at our cozy coffee shop.\"\n",
            "\"Awaken your senses with every sip\"\n",
            "Perk up your day with every sip at our coffee shop.\n",
            "\"More than just a pick-me-up, it's a daily ritual.\"\n",
            "\"Revive your day, one cup at a time!\"\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Basic Prompting\n",
        "The concept of prompting will be introduced here. Learn how to structure input prompts to guide the model's output effectively."
      ],
      "metadata": {
        "id": "7v_OVzS4HZ_e"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Summarisation\n",
        "See how to utilize ChatGPT for text summarization tasks, providing a brief yet comprehensive overview of a larger text."
      ],
      "metadata": {
        "id": "xUeqJ-ygI7t1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Example from DAIR.AI\n",
        "prompt = \"\"\"Antibiotics are a type of medication used to treat bacterial infections.\n",
        "They work by either killing the bacteria or preventing them from reproducing, allowing the body's immune system to fight off the infection.\n",
        "Antibiotics are usually taken orally in the form of pills, capsules, or liquid solutions, or sometimes administered intravenously.\n",
        "They are not effective against viral infections, and using them inappropriately can lead to antibiotic resistance.\n",
        "\n",
        "Explain the above in a single sentence:\"\"\"\n",
        "\n",
        "print(use_endpoint(prompt, 0.7, 1))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cBYCqdiEI61P",
        "outputId": "12618738-2f22-46e4-9e15-a9736a20be05"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Antibiotics are medications used to treat bacterial infections by either killing bacteria or preventing their reproduction, but they are not effective against viral infections and should be used appropriately to avoid antibiotic resistance.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Question-Answering\n",
        "This section will demonstrate how to frame questions to the model in a way that elicits accurate and useful answers."
      ],
      "metadata": {
        "id": "FwGJUT0UJePf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Example from DAIR.AI\n",
        "prompt = \"\"\"Answer the question based on the context below. Keep the answer short and concise. Respond \"Unsure about answer\" if not sure about the answer.\n",
        "\n",
        "Context: Teplizumab traces its roots to a New Jersey drug company called Ortho Pharmaceutical. There, scientists generated an early version of the antibody, dubbed OKT3.\n",
        "Originally sourced from mice, the molecule was able to bind to the surface of T cells and limit their cell-killing potential.\n",
        "In 1986, it was approved to help prevent organ rejection after kidney transplants, making it the first therapeutic antibody allowed for human use.\n",
        "\n",
        "Question: What was OKT3 originally sourced from?\n",
        "\n",
        "Answer:\"\"\"\n",
        "\n",
        "print(use_endpoint(prompt, 0.5, 1))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ek5W-RxoJpE2",
        "outputId": "50165257-78e3-484c-e13b-246979152102"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mice.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Text Classification\n",
        "Learn how to use ChatGPT for classifying text into predefined categories. This showcases the model's ability to transform unstructured text into structured data."
      ],
      "metadata": {
        "id": "9FiC66vFLCV2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Example from DAIR.AI\n",
        "prompt = \"\"\"Classify the text into neutral, negative or positive.\n",
        "\n",
        "Text: I think the food was okay.\n",
        "\n",
        "Sentiment:\"\"\"\n",
        "\n",
        "print(use_endpoint(prompt, 0.7, 1))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ea9dad68-becb-4a11-fb58-93fa83d3bb3e",
        "id": "wRXnyvOpLCV3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Neutral\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "# Advanced Prompting with Zero-shot, One-shot, Few-shot learning\n",
        "This part will dive deeper into the art of prompting. It will explore how to utilize the concepts of zero-shot, one-shot, and few-shot learning to guide the model's output even further. Learn how to provide context through examples to guide the model's behavior."
      ],
      "metadata": {
        "id": "LWC4PoxLAdSy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Zero Shot Learning\n",
        "\n",
        "In zero-shot learning, the model generates an output for a specific task without having seen any explicit examples of that task during its training. The model relies solely on its general understanding of language and the specific prompt it's given. This is a challenging scenario, as the model needs to generalize well beyond its training data. This technique is useful when there are no available training examples for a specific task."
      ],
      "metadata": {
        "id": "GC1HKenFv1qw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "prompt = \"\"\"Show only the empathetic message and nothing else.\n",
        "\n",
        "\n",
        "A customer left a review. We follow up with anyone who appears unhappy.\n",
        "Extract all entities mentioned. For each entity:\n",
        "- classify sentiment as [\"POSITIVE\", \"NEUTRAL\", \"NEGATIVE\"]\n",
        "- whether customer requires a follow-up: Y or N\n",
        "- reason for requiring followup\n",
        "\n",
        "Provide an empathetic message I can\n",
        "send to my customer including the offer to have a call with the relevant\n",
        "product manager to leave feedback. I want to win back their favour and\n",
        "I do not want the customer to churn.\n",
        "\n",
        "Review:\n",
        "The Jivano Crunch Cereal was a huge disappointment. It tasted\n",
        "bland and stale, nothing like the description on the packaging.\n",
        "I wouldn't recommend it to anyone.\"\"\"\n",
        "\n",
        "print(use_endpoint(prompt, 0.5, 1))"
      ],
      "metadata": {
        "id": "OVt1fuvXv1bZ",
        "outputId": "2ac51088-59d9-4547-c2b1-ab056905bcd0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Entity: Jivano Crunch Cereal\n",
            "Sentiment: NEGATIVE\n",
            "Follow-up required: Y\n",
            "Reason for follow-up: Customer expressed disappointment and would not recommend the product.\n",
            "\n",
            "Empathetic message: \n",
            "Dear [Customer Name], \n",
            "We're sorry to hear that you were disappointed with our Jivano Crunch Cereal. We take all feedback seriously and would like to make things right. We would like to offer you a call with our product manager to discuss your experience and any suggestions you may have for improvement. Additionally, we would like to offer you a replacement product or a refund. We value your business and hope to have the opportunity to earn back your trust. \n",
            "Best, [Your Name]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## One Shot Learning\n",
        "\n",
        "One-shot learning refers to the situation where the model is provided with a single example of a task at inference time to guide its output. This single example acts as a point of reference for the model, helping it understand what's expected in the output. One-shot learning is useful when you have a limited number of examples for a specific task."
      ],
      "metadata": {
        "id": "ubXLYAtfAjlr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Few Shot Learning\n",
        "\n",
        "Few-shot learning involves providing the model with a small number of examples of a task at inference time. These examples serve to guide the model's generation and help it produce the desired output. The idea is that the model can generalize from these few examples to understand and complete the task. This technique is valuable when you have more than one but still a limited number of examples for a task."
      ],
      "metadata": {
        "id": "DOH9ufb4TvmJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Basically we are giving some input-output pairs and improve the results. This is the primary fine-tuning method of GPT."
      ],
      "metadata": {
        "id": "pdWNUp4FZDth"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "With a basic prompt using ChatGPT."
      ],
      "metadata": {
        "id": "-a8jNtTmZNcz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "prompt = \"\"\"The odd numbers in this group add up to an even number: 15, 32, 5, 13, 82, 7, 1.\n",
        "Is this statement True or False?\"\"\"\n",
        "\n",
        "print(use_endpoint(prompt))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hw-yqAH_B__p",
        "outputId": "dc4d9598-9ee0-49e3-f946-9ac3f1ba8b30"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "This statement is True. \n",
            "\n",
            "The odd numbers in this group are: 15, 5, 13, 7, and 1. \n",
            "\n",
            "Their sum is 15 + 5 + 13 + 7 + 1 = 41 \n",
            "\n",
            "41 is an odd number. \n",
            "\n",
            "Therefore, the statement \"the odd numbers in this group add up to an even number\" is false. \n",
            "\n",
            "However, if we take all the numbers in the group and remove the even numbers, we are left with the odd numbers mentioned earlier. \n",
            "\n",
            "The even numbers in the group are: 32 and 82. \n",
            "\n",
            "Their sum is 32 + 82 = 114 \n",
            "\n",
            "114 is an even number. \n",
            "\n",
            "Therefore, the statement \"the even numbers in this group add up to an even number\" is true.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Basic prompt using GPT-3 model."
      ],
      "metadata": {
        "id": "VqAnIXLZZlxx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The odd numbers here should add up to 41 and the total numbers should add up to 155."
      ],
      "metadata": {
        "id": "Tzv8ZpoJG0oP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "prompt = \"\"\"The odd numbers in this group add up to an even number: 15, 32, 5, 13, 82, 7, 1.\"\"\"\n",
        "\n",
        "print(use_endpoint(prompt,model=\"text-davinci-003\"))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c97e9475-4f97-4254-8f36-4cec7c5d3d71",
        "id": "7a57K4bco0U2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The odd numbers add up to 118, which is an even number.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "With few shot learning."
      ],
      "metadata": {
        "id": "vPTsE1zLZ1NH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "prompt = \"\"\"The odd numbers in this group add up to an even number: 4, 8, 9, 15, 12, 2, 1.\n",
        "A: The answer is False.\n",
        "\n",
        "The odd numbers in this group add up to an even number: 17,  10, 19, 4, 8, 12, 24.\n",
        "A: The answer is True.\n",
        "\n",
        "The odd numbers in this group add up to an even number: 16,  11, 14, 4, 8, 13, 24.\n",
        "A: The answer is True.\n",
        "\n",
        "The odd numbers in this group add up to an even number: 17,  9, 10, 12, 13, 4, 2.\n",
        "A: The answer is False.\n",
        "\n",
        "The odd numbers in this group add up to an even number: 15, 32, 5, 13, 82, 7, 1.\n",
        "A:\"\"\"\n",
        "\n",
        "print(use_endpoint(prompt))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xv3VGLQSHpNs",
        "outputId": "5a470b69-f5c2-4b0a-fa46-28b92cbd8aeb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The answer is False.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "With Zero-shot Chain of thought."
      ],
      "metadata": {
        "id": "dI2e3bdtajqC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ask LLM to think in steps."
      ],
      "metadata": {
        "id": "cOS7YYjIbcYN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Basic prompt using GPT-3 model."
      ],
      "metadata": {
        "id": "YIy4VhNPaexK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The answer here should be 10."
      ],
      "metadata": {
        "id": "WLBQlE_AHdAn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "prompt = \"\"\"I went to the market and bought 10 apples. I gave 2 apples to the neighbor and 2 to the repairman. I then went and bought 5 more apples and ate 1. How many apples did I remain with?\"\"\"\n",
        "\n",
        "print(use_endpoint(prompt,model=\"text-davinci-003\"))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b924f745-bbcb-4602-f98f-3bd2f5e397df",
        "id": "UrGwGG5hrsCS"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "11 apples\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "prompt = \"\"\"I went to the market and bought 10 apples. I gave 2 apples to the neighbor and 2 to the repairman. I then went and bought 5 more apples and ate 1. How many apples did I remain with?\n",
        "\n",
        "Let's think step by step.\"\"\"\n",
        "\n",
        "print(use_endpoint(prompt,model=\"text-davinci-003\"))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1c871cd6-dc22-4980-a071-7ecbde02224a",
        "id": "v3RSCA4pr1ZN"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Initially you had 10 apples.\n",
            "You gave away 4 apples, so you have 6 apples remaining.\n",
            "You bought 5 more apples, so you have 11 apples remaining.\n",
            "You ate 1 apple, so you are left with 10 apples.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "ChatGPT latest version is improved with CoT itself."
      ],
      "metadata": {
        "id": "sInIr1zhsWFG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "prompt = \"\"\"I went to the market and bought 10 apples. I gave 2 apples to the neighbor and 2 to the repairman. I then went and bought 5 more apples and ate 1. How many apples did I remain with?\"\"\"\n",
        "\n",
        "print(use_endpoint(prompt))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Yo2omdkFedGw",
        "outputId": "637c0f36-de7d-4aa5-e126-d9f5dfddaf34"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "You remained with 10 - 2 - 2 + 5 - 1 = 10 apples.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Few Shot Learning another perspective"
      ],
      "metadata": {
        "id": "pM6_axHV0wRm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Using ChatGPT we can get correct answers with basic prompting even."
      ],
      "metadata": {
        "id": "yotDD2lncb1e"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "prompt = \"\"\"When I was 6 my sister was half my age. Now I’m 70 how old is my sister?\"\"\"\n",
        "print(use_endpoint(prompt,model=\"text-davinci-003\"))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3bfc57cb-9a48-449f-a99f-133c8322e9d7",
        "id": "S3lgn1FHslDU"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "35 years old.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "prompt = \"\"\"Q: There are 15 trees in the grove. Grove workers will plant trees in the grove today. After they are done,\n",
        "there will be 21 trees. How many trees did the grove workers plant today?\n",
        "A: We start with 15 trees. Later we have 21 trees. The difference must be the number of trees they planted.\n",
        "So, they must have planted 21 - 15 = 6 trees. The answer is 6.\n",
        "\n",
        "Q: If there are 3 cars in the parking lot and 2 more cars arrive, how many cars are in the parking lot?\n",
        "A: There are 3 cars in the parking lot already. 2 more arrive. Now there are 3 + 2 = 5 cars. The answer is 5.\n",
        "\n",
        "Q: Leah had 32 chocolates and her sister had 42. If they ate 35, how many pieces do they have left in total?\n",
        "A: Leah had 32 chocolates and Leah’s sister had 42. That means there were originally 32 + 42 = 74\n",
        "chocolates. 35 have been eaten. So in total they still have 74 - 35 = 39 chocolates. The answer is 39.\n",
        "\n",
        "Q: Jason had 20 lollipops. He gave Denny some lollipops. Now Jason has 12 lollipops. How many lollipops\n",
        "did Jason give to Denny?\n",
        "A: Jason had 20 lollipops. Since he only has 12 now, he must have given the rest to Denny. The number of\n",
        "lollipops he has given to Denny must have been 20 - 12 = 8 lollipops. The answer is 8.\n",
        "\n",
        "Q: Shawn has five toys. For Christmas, he got two toys each from his mom and dad. How many toys does\n",
        "he have now?\n",
        "A: He has 5 toys. He got 2 from mom, so after that he has 5 + 2 = 7 toys. Then he got 2 more from dad, so\n",
        "in total he has 7 + 2 = 9 toys. The answer is 9.\n",
        "\n",
        "Q: There were nine computers in the server room. Five more computers were installed each day, from\n",
        "monday to thursday. How many computers are now in the server room?\n",
        "A: There are 4 days from monday to thursday. 5 computers were added each day. That means in total 4 * 5 =\n",
        "20 computers were added. There were 9 computers in the beginning, so now there are 9 + 20 = 29 computers.\n",
        "The answer is 29.\n",
        "\n",
        "Q: Michael had 58 golf balls. On tuesday, he lost 23 golf balls. On wednesday, he lost 2 more. How many\n",
        "golf balls did he have at the end of wednesday?\n",
        "A: Michael initially had 58 balls. He lost 23 on Tuesday, so after that he has 58 - 23 = 35 balls. On\n",
        "Wednesday he lost 2 more so now he has 35 - 2 = 33 balls. The answer is 33.\n",
        "\n",
        "Q: Olivia has $23. She bought five bagels for $3 each. How much money does she have left?\n",
        "A: She bought 5 bagels for $3 each. This means she spent 5\n",
        "\n",
        "Q: When I was 6 my sister was half my age. Now I’m 70 how old is my sister?\n",
        "A:\"\"\"\n",
        "print(use_endpoint(prompt,model=\"text-davinci-003\"))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "538b1b4a-c0df-45f4-b83a-0e26194b1dd8",
        "id": "u59yBWmQbh2Z"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "When the person was 6, the sister was half their age, so the sister must have been 3. If the person is now 70, then the sister is 70 - 6 = 64 years old. The answer is 64.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "prompt = \"\"\"When I was 6 my sister was half my age. Now I’m 70 how old is my sister?\"\"\"\n",
        "print(use_endpoint(prompt))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tXGn5atpnp1u",
        "outputId": "4bd5d26c-e1f6-42fb-f9f9-656cff7ab325"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Your sister is 67 years old.\n"
          ]
        }
      ]
    }
  ]
}